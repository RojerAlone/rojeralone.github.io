<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ostep on RojerAlone</title><link>https://rojeralone.github.io/tags/ostep/</link><description>Recent content in ostep on RojerAlone</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 12 Aug 2021 19:40:09 +0800</lastBuildDate><atom:link href="https://rojeralone.github.io/tags/ostep/index.xml" rel="self" type="application/rss+xml"/><item><title>OSTEP 读书笔记: 文件系统实现</title><link>https://rojeralone.github.io/ostep/file-system-implement/</link><pubDate>Thu, 12 Aug 2021 19:40:09 +0800</pubDate><guid>https://rojeralone.github.io/ostep/file-system-implement/</guid><description>文件和文件夹 操作系统对文件系统的关键抽象就是文件和文件夹。
文件是线性字节数组组成的一个整体，用户可以进行读写。每个文件还有一个低级别的名字，通常用一个数字表示，用户一般对这个名字无感知，由于历史原因，这个名字通常叫做 inode。
文件夹和文件类似，也有一个 inode，但是它的内容不是二进制字节数据，而是包含了文件夹和文件的列表，即它只是容器，不直接保存数据，而是保存其他文件夹和文件。文件系统通常有个 root directory，在 Unix 系统中是 /。通过 / 分割多层级的文件夹，可以得到一个文件夹树，文件夹树中的每个文件或文件夹都可以用路径表示，文件系统也通过路径操作对应的文件或文件夹。
文件系统接口 create open() 命令打开文件，通过传入 O_CREATE 参数创建文件，open 命令返回一个数字，称为 fd (file descriptor)，fd 是进程私有的，对文件的操作都通过 fd 进行，每个进程内部都有一个 open file table 表示打开的文件。
read and write 上面说到对文件的操作要通过 fd 进行，读写文件的指令都需要传入 fd，read 和 write 返回读写了多少字节的数据。
strace 命令可以跟踪操作进行的系统调用，通过 strace cat xxx 命令，看到 cat 命令中间会执行 open、read、write 等调用，发现 open 返回的 fd 是 3。一个进程默认会有 3 个 fd，标准输入、标准输出和错误输出，这 3 种 IO 占用了 0 / 1 / 2 这 3 个 fd，所以第一个用户主动打开的 fd 是 3。</description></item><item><title>OSTEP 读书笔记: 并发之条件变量和信号量</title><link>https://rojeralone.github.io/ostep/concurrency-condition-variable-and-semaphore/</link><pubDate>Sun, 01 Aug 2021 17:39:19 +0800</pubDate><guid>https://rojeralone.github.io/ostep/concurrency-condition-variable-and-semaphore/</guid><description>条件变量 线程可以使用一个变量来表示条件，当这个条件为 true 时，线程可以执行，否则需要等待，这就是条件变量。
POSIX 定义了条件变量的原语：pthread_cond_wait 和 pthread_cond_signal，通常叫做 wait() 和 signal()。
一般条件变量和锁一起使用，pthread_cond_wait 的参数就有个锁变量，持有锁的线程调用 wait() 释放锁，并等待唤醒，获取锁的线程调用 signal() 唤醒等待的线程，被唤醒的线程需要等待锁被释放，并且自己获取到锁以后再继续往下执行。
为了更好理解条件变量的含义和用法，使用生产/消费者模型，或者叫有界缓冲区来帮助读者掌握。
生产/消费者问题 初始版本是非常粗糙的实现，缓冲区只能存放一个元素，put / get 时分别需要判断队列是否满了或者空了。
问题1：生产者消费者两个线程执行，很容易就遇到不满足判断条件的情况。
优化1：使用条件变量。生产/消费前先获取锁，然后判断缓冲区的容量，如果不满足要求，就释放锁，等待条件变量满足后被唤醒。
这样的优化在生产者和消费者都是单线程的情况下可以 work，但是如果消费者有 2 个，就会出现这样的情况：消费者 A 消费完后，唤醒了消费者 B，B 去消费，发现没有数据，get 失败了。
问题2: 如果有多个消费者会出现被唤醒后，获取不到数据的情况
优化2: 判断满足条件时，应该使用 while 循环判断，而不是 if，也就是 wait 等待的代码块应该在循环中，不是在 if 代码块中，这样被唤醒后再次判断条件是否满足，不满足继续睡眠等待。
优化后还是有些小问题，当消费者消费完成后，唤醒的是另一个消费者，它发现队列是空的，就又陷入睡眠，这样并不是效率很高的执行方式，正确的操作是唤醒生产者，让它继续往队列里生产数据。
问题3: 效率低，消费空队列后没唤醒生产者
优化3: 使用两个条件变量，消费者等待队列中有数据的条件变量，生产者等待队列不满的条件变量。
当缓冲区的大小只有 1 时，效率不是那么高，正常情况下也不会消费队列不会这么小。
问题4: 缓冲区小，性能不够高
优化4: 增大缓冲区，使用变量记录缓冲区大小，当缓冲区没满时生产者就一直生产，缓冲区不空消费者就一直消费。
唤醒所有线程 上述问题 3 还有另一种解法，只用一个条件变量， 但是唤醒等待线程时不是只唤醒一个，而是唤醒所有的等待线程，这样就一定能够唤醒该唤醒的线程，不该唤醒的线程会影响性能，它们只检查条件，然后又陷入沉睡，这种能覆盖需要唤醒线程的场景叫做覆盖条件。
一些想法 书中描述条件变量的唤醒时，只是说唤醒睡眠的线程，并没有详细描述怎么实现的，根据查资料以及个人的理解，signal() 并不是”真正“唤醒了线程，而是把线程从一个不可被调度器调度的状态，转为了可执行，但是在等待锁的状态，一般是从一个等待队列换到了竞争锁的队列中，直到调用 signal() 的线程释放锁，等待锁的线程竞争锁，如果被”唤醒“的线程竞争到了锁，那么它才真正被唤醒，继续接着 wait() 之后的代码执行，否则继续等待竞争锁。
信号量 信号量是一个有计数的变量，sem_wait() 和 sem_post() 是操作信号量的两个指令，信号量初始化时可以执行计数的值，当计数值为负数时，表示有多少个线程在等待信号，否则，线程获取到了信号，可以立马执行。</description></item><item><title>OSTEP 读书笔记: 并发之锁</title><link>https://rojeralone.github.io/ostep/concurrency-lock/</link><pubDate>Mon, 19 Jul 2021 23:27:08 +0800</pubDate><guid>https://rojeralone.github.io/ostep/concurrency-lock/</guid><description>评估 如果评估锁的好坏？
是否能够做到互斥 公平性，避免饥饿 性能 实现方式 控制中断 最简单的实现方式，当执行需要互斥的代码时，禁止中断。
问题：
禁止中断是特权操作，普通进程能执行这个操作比较危险 多处理器时，这种方式就不管用了 关掉中断可能导致比较重要的中断无法响应，甚至导致系统崩溃 性能很差 Loads / Stores 用一个变量标识是否获取了锁，问题是，两个线程同时执行 if (flag == 0) flag = 1 这个操作，这不是个原子操作，可能会被中断，A 准备赋值为 1 时被中断了，导致另一个线程认为自己应该获取锁，那么两个线程都能获取到锁。
Test-And-Set 自旋锁 硬件提供了 Test-And-Set 指令，对某个变量设置新值的同时返回旧值，操作系统在这个指令的基础上实现锁的概念。
这个实现方式可以保证互斥，但是不能保证公平，性能也比较差，存在浪费 CPU 的情况（一直循环执行 Test-And-Set）。
另外，一些 CPU 也提供了另一种硬件支持的原语，Compare-And-Set CAS，也能实现锁。
还有其他的硬件支持的指令，比如 Fetch-And-Add
Just Yield 在自旋锁的基础上优化性能，对于没获取到锁的线程，让出自己的 CPU，然后等待已经获取锁的线程使用完后释放锁，主动唤醒正在等待的线程。
问题是，并不能解决饥饿问题，存在线程反复获取锁然后释放，导致另一些线程一直等待的问题。
等待队列 解决上一种实现方式的问题，没获取到锁的线程排队进入一个队列里，获取了锁的线程释放锁时，唤醒队列中第一个线程，避免无尽等待。需要操作系统提供支持，park() 用来让出 CPU 并睡眠，unpark(pid) 用来唤醒指定线程。
两阶段锁 进一步优化，第一阶段尝试自旋获取锁，如果在执行时间内没有获取到锁，再进入等待队列。
锁相关的数据结构 并发计数器 多个线程同时执行加法计算，由于要保证并发安全，需要加锁，那么多线程执行时的速度就很慢。理论上，多线程应该要比单线程要快，实际可能并非如此。
通过多核 CPU 同时执行，能够提升相应的性能，叫做完美扩展，比如原来单个 CPU 需要 1s 完成的任务，任务量扩大了 3 倍，3 个 CPU 同时执行，还是只需要 1s 就能完成。</description></item><item><title>OSTEP 读书笔记: 内存虚拟化之内存页</title><link>https://rojeralone.github.io/ostep/virtual-memory-page/</link><pubDate>Sun, 11 Jul 2021 22:17:40 +0800</pubDate><guid>https://rojeralone.github.io/ostep/virtual-memory-page/</guid><description>如果将内存以定长的分页管理，那么就能解决分段管理内存的碎片化问题，并且管理空闲内存也更方便，只需要一个空闲 page 列表，需要多大内存就分配相应个数的 page 就可以了。
为了简化虚拟内存到物理内存的映射关系，分页的大小是固定的，虚拟内存和物理内存都使用相同大小的分页。
操作系统使用页表（page table) 来表示虚拟内存到物理内存的映射，并且每个进程都有自己的页表。
每个虚拟内存页都有一个编号来表示，称为 VPN (Vitrual Page Number)，而物理内存页的编号为 PFN (Physical Frame Number) 或者 PPN (Physical Page Number)。
为了知道访问的虚拟内存是哪个页，虚拟地址需要耗费几个 bit 来表示，比如高 2 位表示 VPN，剩下的表示在内存页内的偏移量，操作系统只需要查找页表中对应的 VPN 映射的 PPN，然后访问指定偏移量的内存就行了。
页表存储 假设在 32-bit 的系统上，page 大小为 4K，那么一个进程的地址空间为 4GB，有 1M 个 page，每个 page 的数据结构 PTE (Page Table Entry) 需要 4 byte，那么每个进程需要 4MB 的内存来保存页表，1000 个进程就需要 4GB 的内存。
由于页表太大了，没法直接在 MMU 中存储，只能放在内存里(也可以放在磁盘)，那么现在的问题就是解决页表的存储问题，既要减少内存使用，又要高效查找映射关系。
PTE 中有什么？
valid bit：有效位，内存是否已经分配给进程了，没分配的不能访问 protection bit：保护位，是否可读 / 可写 / 可执行 persent bit：持久位，是否已经置换到磁盘上了 dirty bit：从磁盘加载到内存以后是否被修改过，没修改的话下次置换它就不用写磁盘了 access bit：记录访问信息，用来决定是否是热点数据，能不能被置换掉 &amp;hellip; 另一个问题就是效率，如果要访问内存，操作系统本身要解析虚拟地址对应的 VPN，然后访问页表，找到映射关系，也就是寻址这个操作中不是一步完成的，还需要另一次寻址。</description></item><item><title>OSTEP 读书笔记: 内存虚拟化之基础知识</title><link>https://rojeralone.github.io/ostep/virtual-memory-basic/</link><pubDate>Wed, 23 Jun 2021 20:14:15 +0800</pubDate><guid>https://rojeralone.github.io/ostep/virtual-memory-basic/</guid><description>内存虚拟化 - 基础知识 地址空间 操作系统为虚拟内存提供的抽象叫做地址空间(Address Space)，是运行中的进程能够看到的内存空间。进程申请内存主要用作 3 种数据的存储：代码、堆、栈。每个进程看到的地址空间都是一样的，给进程一种自己拥有所有内存的假象，而在真正的物理内存上，实现了进程隔离，保证了内存操作的安全性。
目标 内存虚拟化的目标：
透明 高效 保护：进程之间不能互相访问对方的内存空间 内存操作 API 一个进程会显示申请两种类型的内存，栈和堆。栈内存由编译器隐式帮程序员申请了，并且在生命周期结束后隐式释放，如果要将该内存在生命周期之外使用，则要分配堆内存，内存管理主要关注堆内存。
malloc() void *malloc(size_t size) 申请大小为 size 的内存空间，返回指向这块内存空间的指针，一般使用类型转换转化为目标类型。size 一般使用 sizeof() 操作获取，因为在不同的平台上大小可能不一样，编译器会解析 sizeof() 的结果，而不是在运行时才解析。
free() 释放内存，把 malloc 返回的指针传进去就行，内存分配器会知道释放多少内存的，不需要显示传进去。
使用注意事项 未分配内存：一些函数的指针参数是非空指针，比如 strcpy 未分配足够的内存：还是 strcpy，报错与否是不确定的 未初始化申请的内存：里边的内容是不确定的 未释放申请的内存：导致内存泄漏 提前释放了还在使用的内存：野指针 / 悬浮指针 重复释放内存：不可预知的后果，大部分是崩溃 错误调用 free(): 释放了未知的内存 系统调用 malloc() 和 free() 并不是系统调用，而是内存分配库提供的功能，这些库的底层依赖了系统调用 brk 和 sbrk，程序员不应该直接调用这些方法，除非你很清楚你在做什么，另外还可以通过 mmap 来分配内存，这是个系统调用。
内存分配库还提供一些其他调用，比如 calloc() 分配全是 0 的内存，realloc() 传入一个内存指针，返回一个更大的但是已经将旧内存 copy 过去的新内存地址。
地址转换 内存的虚拟化和 CPU 的虚拟化追求的目标一致：效率和控制。</description></item><item><title>OSTEP 读书笔记: 虚拟化之多处理器任务调度</title><link>https://rojeralone.github.io/ostep/multiprocessor-scheduler/</link><pubDate>Mon, 21 Jun 2021 11:06:29 +0800</pubDate><guid>https://rojeralone.github.io/ostep/multiprocessor-scheduler/</guid><description>Multiprocessors Scheduling 之前讨论的都是单 CPU 上的多个任务调度，但是现代 CPU 大都是多核 CPU，多线程编程也随之出现，编程方式也和以前不一样，这里就讨论下多核调度策略。
多核带来的第一个问题是缓存的一致性问题， CPU 会缓存一些数据到 CPU 核中，多核情况下，一个任务从 A 核调度到了 B 核，那么在修改了值以后可能读到缓存的旧值，可以通过总线来解决，每个 CPU 核监听总线上的时间，当发现缓存数据对应的内存中数据变化时，标记脏数据或更新数据。
另一个问题是并发问题，最后一个问题是缓存亲缘性，一个任务最好一直在同一个 CPU 处理器上运行，这样就能使用之前的缓存了。
SQMS single-queue multiprocessor scheduling 的实现方式是将所有的任务都在同一个队列中，多处理器调度时都从这个队列中进行调度，优点是能够在之前提到的调度器上稍微修改就能运行。
缺点也很明显，多处理器消费一个队列，要加锁才能保证并发安全， 而当处理器核数很多的时候，性能就会变差。另一个问题是缓存亲缘性，这个问题也可以解决，通过忽略一些在其他处理器上运行的任务，但是实现起来比较复杂。
MQMS 很自然想到，一个任务队列有问题，那么多个任务队列呢？这就是 multi-queue multiprocessor scheduling。 它同时解决了并发和缓存亲缘性问题。
MQMS 的问题是负载均衡，当任务放到一个 CPU 核的队列内后不再变化，任务提交时任务的执行时间不可预测，会出现某个 CPU 的任务全部完成了，另一个 CPU 还要运行好久才能完成任务。
这时候就不得不放弃一些缓存亲缘性了，将一些等待执行的任务迁移到空闲的 CPU 上。一种实现机制是工作窃取，一个 CPU 上的调度器偶尔会偷看另一个 CPU 上的调度负载，如果发现另一个 CPU 比较繁忙而自己比较空闲，那么就“偷”一个或一些任务来运行。这里又有一个权衡，不能太频繁偷看，否则消耗太高，这又是一个调参找阈值的地方。
Linux Multiprocessor Schedulers Linux 的调度器没有采用统一的多队列，O(1) 和 CFS 调度器都采用了多队列，但是 BFS 调度器并没有，它采用了单队列。书中并没有详细说明这些调度器的策略，下面是维基的页面：
O(1) Scheduler BFS</description></item><item><title>OSTEP 读书笔记: 虚拟化之任务调度</title><link>https://rojeralone.github.io/ostep/scheduler/</link><pubDate>Sun, 20 Jun 2021 15:33:09 +0800</pubDate><guid>https://rojeralone.github.io/ostep/scheduler/</guid><description>虚拟化之 CPU 虚拟化：任务调度 工作负载假设 任务运行时间相同 任务同时到达开始运行 任务开始运行不停止直到运行结束 所有任务只需要用 CPU 运行时间已知 调度指标 平均等待时间
调度策略 FIFO，放开假设 1，导致问题：耗时长的任务先运行，一些短任务要等很久 SJF（Shortest Job First）：解决 FIFO 的弊端，放开假设 2，长任务执行时短任务来了，也要等很久 STCF（Shortest Time-to-Completion First）：放开假设 3，允许中断，可以先完成的任务先执行 现在引入新的指标，响应时间，即任务开始运行的时间减去任务到达的时间，因为操作系统会有用户直接在控制台执行命令的场景，希望立马能够有响应而不是给人系统卡住的感觉。
Round Robin 算法，轮转执行，以时钟周期的整数倍为一个时间片，每个任务轮流执行一个时间片，这样能够降低响应时间，但是完成等待时间就上去了，鱼与熊掌不可兼得。 那么放开假设 4，任务需要执行 IO，策略变更为当任务执行 IO 时，让出 CPU 给需要的任务。
在真实的系统中，运行时间是很不确定的，很难预测，应该怎么处理？下面的多级反馈队列机制将会解决这个问题。
MLFQ MLFQ (Multiple-Level Feedback Queue) 的基本原理是有多个优先级队列，优先级更高的队列里的任务优先执行，优先级相同的就轮流执行，同时还会动态调整任务的优先级。
Rule 1： A 的优先级大于 B，则 A 运行 Rule 2：A 优先级等于 B，A、B 使用 RR 运行 Rule 3：任务刚提交的时候，优先级默认为最高优先级 Rule 4a：任务运行时用光了一个完整的时间片，则降低优先级，因为它是 CPU 密集的，对响应时间不敏感 Rule 4b：用完时间片之前让出了 CPU，保持优先级不变 MLFQ 有几个问题：饥饿、投机、优先级只降不升</description></item></channel></rss>