<!doctype html><html lang=en-us><head><title>OSTEP 读书笔记: 内存虚拟化之内存页 // RojerAlone</title><link rel="shortcut icon" href=/favicon.ico><meta charset=utf-8><meta name=generator content="Hugo 0.81.0"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="RojerAlone"><meta name=description content><link rel=stylesheet href=https://rojeralone.cn/css/main.min.88e7083eff65effb7485b6e6f38d10afbec25093a6fac42d734ce9024d3defbd.css><meta name=twitter:card content="summary"><meta name=twitter:title content="OSTEP 读书笔记: 内存虚拟化之内存页"><meta name=twitter:description content="如果将内存以定长的分页管理，那么就能解决分段管理内存的碎片化问题，并且管理空闲内存也更方便，只需要一个空闲 page 列表，需要多大内存就分配相应个数的 page 就可以了。
为了简化虚拟内存到物理内存的映射关系，分页的大小是固定的，虚拟内存和物理内存都使用相同大小的分页。
操作系统使用页表（page table) 来表示虚拟内存到物理内存的映射，并且每个进程都有自己的页表。
每个虚拟内存页都有一个编号来表示，称为 VPN (Vitrual Page Number)，而物理内存页的编号为 PFN (Physical Frame Number) 或者 PPN (Physical Page Number)。
为了知道访问的虚拟内存是哪个页，虚拟地址需要耗费几个 bit 来表示，比如高 2 位表示 VPN，剩下的表示在内存页内的偏移量，操作系统只需要查找页表中对应的 VPN 映射的 PPN，然后访问指定偏移量的内存就行了。
页表存储 假设在 32-bit 的系统上，page 大小为 4K，那么一个进程的地址空间为 4GB，有 1M 个 page，每个 page 的数据结构 PTE (Page Table Entry) 需要 4 byte，那么每个进程需要 4MB 的内存来保存页表，1000 个进程就需要 4GB 的内存。
由于页表太大了，没法直接在 MMU 中存储，只能放在内存里(也可以放在磁盘)，那么现在的问题就是解决页表的存储问题，既要减少内存使用，又要高效查找映射关系。
PTE 中有什么？
 valid bit：有效位，内存是否已经分配给进程了，没分配的不能访问 protection bit：保护位，是否可读 / 可写 / 可执行 persent bit：持久位，是否已经置换到磁盘上了 dirty bit：从磁盘加载到内存以后是否被修改过，没修改的话下次置换它就不用写磁盘了 access bit：记录访问信息，用来决定是否是热点数据，能不能被置换掉 &mldr;  另一个问题就是效率，如果要访问内存，操作系统本身要解析虚拟地址对应的 VPN，然后访问页表，找到映射关系，也就是寻址这个操作中不是一步完成的，还需要另一次寻址。"><meta property="og:title" content="OSTEP 读书笔记: 内存虚拟化之内存页"><meta property="og:description" content="如果将内存以定长的分页管理，那么就能解决分段管理内存的碎片化问题，并且管理空闲内存也更方便，只需要一个空闲 page 列表，需要多大内存就分配相应个数的 page 就可以了。
为了简化虚拟内存到物理内存的映射关系，分页的大小是固定的，虚拟内存和物理内存都使用相同大小的分页。
操作系统使用页表（page table) 来表示虚拟内存到物理内存的映射，并且每个进程都有自己的页表。
每个虚拟内存页都有一个编号来表示，称为 VPN (Vitrual Page Number)，而物理内存页的编号为 PFN (Physical Frame Number) 或者 PPN (Physical Page Number)。
为了知道访问的虚拟内存是哪个页，虚拟地址需要耗费几个 bit 来表示，比如高 2 位表示 VPN，剩下的表示在内存页内的偏移量，操作系统只需要查找页表中对应的 VPN 映射的 PPN，然后访问指定偏移量的内存就行了。
页表存储 假设在 32-bit 的系统上，page 大小为 4K，那么一个进程的地址空间为 4GB，有 1M 个 page，每个 page 的数据结构 PTE (Page Table Entry) 需要 4 byte，那么每个进程需要 4MB 的内存来保存页表，1000 个进程就需要 4GB 的内存。
由于页表太大了，没法直接在 MMU 中存储，只能放在内存里(也可以放在磁盘)，那么现在的问题就是解决页表的存储问题，既要减少内存使用，又要高效查找映射关系。
PTE 中有什么？
 valid bit：有效位，内存是否已经分配给进程了，没分配的不能访问 protection bit：保护位，是否可读 / 可写 / 可执行 persent bit：持久位，是否已经置换到磁盘上了 dirty bit：从磁盘加载到内存以后是否被修改过，没修改的话下次置换它就不用写磁盘了 access bit：记录访问信息，用来决定是否是热点数据，能不能被置换掉 &mldr;  另一个问题就是效率，如果要访问内存，操作系统本身要解析虚拟地址对应的 VPN，然后访问页表，找到映射关系，也就是寻址这个操作中不是一步完成的，还需要另一次寻址。"><meta property="og:type" content="article"><meta property="og:url" content="https://rojeralone.cn/ostep/virtual-memory-page/"><meta property="article:section" content="ostep"><meta property="article:published_time" content="2021-07-11T22:17:40+08:00"><meta property="article:modified_time" content="2021-07-11T22:17:40+08:00"></head><body><header class=app-header><a href=https://rojeralone.cn/><img class=app-header-avatar src=/avatar.jpg alt=RojerAlone></a><h1>RojerAlone</h1><p>Developer</p><div class=app-header-social><a href=https://github.com/RojerAlone target=_blank rel="noreferrer noopener"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-github"><title>Github</title><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></div></header><main class=app-container><article class=post><header class=post-header><h1 class=post-title>OSTEP 读书笔记: 内存虚拟化之内存页</h1><div class=post-meta><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-calendar"><title>calendar</title><rect x="3" y="4" width="18" height="18" rx="2" ry="2"/><line x1="16" y1="2" x2="16" y2="6"/><line x1="8" y1="2" x2="8" y2="6"/><line x1="3" y1="10" x2="21" y2="10"/></svg>Jul 11, 2021</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-clock"><title>clock</title><circle cx="12" cy="12" r="10"/><polyline points="12 6 12 12 16 14"/></svg>2 min read</div><div><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tag"><title>tag</title><path d="M20.59 13.41l-7.17 7.17a2 2 0 01-2.83.0L2 12V2h10l8.59 8.59a2 2 0 010 2.82z"/><line x1="7" y1="7" x2="7.01" y2="7"/></svg><a class=tag href=https://rojeralone.cn/tags/ostep/>ostep</a>
<a class=tag href=https://rojeralone.cn/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/>操作系统</a></div></div></header><div class=post-content><p>如果将内存以定长的分页管理，那么就能解决分段管理内存的碎片化问题，并且管理空闲内存也更方便，只需要一个空闲 page 列表，需要多大内存就分配相应个数的 page 就可以了。</p><p>为了简化虚拟内存到物理内存的映射关系，分页的大小是固定的，虚拟内存和物理内存都使用相同大小的分页。</p><p>操作系统使用页表（<strong>page table</strong>) 来表示虚拟内存到物理内存的映射，并且每个进程都有自己的页表。</p><p>每个虚拟内存页都有一个编号来表示，称为 <strong>VPN</strong> (Vitrual Page Number)，而物理内存页的编号为 <strong>PFN</strong> (Physical Frame Number) 或者 <strong>PPN</strong> (Physical Page Number)。</p><p>为了知道访问的虚拟内存是哪个页，虚拟地址需要耗费几个 bit 来表示，比如高 2 位表示 VPN，剩下的表示在内存页内的偏移量，操作系统只需要查找页表中对应的 VPN 映射的 PPN，然后访问指定偏移量的内存就行了。</p><h2 id=页表存储>页表存储</h2><p>假设在 32-bit 的系统上，page 大小为 4K，那么一个进程的地址空间为 4GB，有 1M 个 page，每个 page 的数据结构 <strong>PTE</strong> (Page Table Entry) 需要 4 byte，那么每个进程需要 4MB 的内存来保存页表，1000 个进程就需要 4GB 的内存。</p><p>由于页表太大了，没法直接在 MMU 中存储，只能放在内存里(也可以放在磁盘)，那么现在的问题就是解决页表的存储问题，既要减少内存使用，又要高效查找映射关系。</p><p>PTE 中有什么？</p><ul><li>valid bit：有效位，内存是否已经分配给进程了，没分配的不能访问</li><li>protection bit：保护位，是否可读 / 可写 / 可执行</li><li>persent bit：持久位，是否已经置换到磁盘上了</li><li>dirty bit：从磁盘加载到内存以后是否被修改过，没修改的话下次置换它就不用写磁盘了</li><li>access bit：记录访问信息，用来决定是否是热点数据，能不能被置换掉</li><li>&mldr;</li></ul><p>另一个问题就是效率，如果要访问内存，操作系统本身要解析虚拟地址对应的 VPN，然后访问页表，找到映射关系，也就是寻址这个操作中不是一步完成的，还需要另一次寻址。</p><h2 id=tlb>TLB</h2><p>TLB (translation-lookaside buffer) 是 MMU 芯片的一部分，可以看做是 VPN 到 PFN 映射关系的硬件级别缓存，用来加速地址转换，更贴切的名字是 address-translation cache，TLB 只是历史原因。</p><p>TLB 命中映射关系，能够极大降低访问内存的耗时，它的位置离 CPU 核很近，并且特意设计，使得访问很快，TLB missing 将会降低性能，毕竟访问内存相比于 CPU 指令的执行太慢了，正是 TLB 的存在，使得虚拟内存真正高效。</p><h3 id=谁来处理-tlb-missing>谁来处理 TLB Missing</h3><p>两种选择，硬件或者操作系统，以前的 CISC (complex instruction sets computer) 复杂指令集计算机使用硬件来处理，它提供了足够复杂 的指令，而现代的计算机使用 SISC (reduced instruction sets computer) 精简指令集，由操作系统来处理。</p><p>SISC 遇到 TLB Missing 将会抛出异常，操作系统来处理异常，查找页表然后用特权指令写入 TLB，然后返回，硬件重新执行中断的指令。</p><p>需要注意的是，硬件从中断返回以后，不是像普通进程那样直接继续执行，而是要回到中断前的指令，因此需要特殊地修改 PC。另外一点是，既然已经遇到了 TLB Missing，操作系统去查页表的时候，会再次遇到 TLB，然后死循环了，解决方式是操作系统直接把页表放在物理内存里，这样就不需要 TLB 了，或者直接将页表这块内存的映射关系永久(操作系统运行期间)保存在 TLB 中，这样就一直能够命中。</p><p>操作系统处理 TLB Missing 的优势是灵活性，可以以任意高效的数据结构来查询页表，不需要适配硬件，另一个优势是简化的硬件的逻辑，使硬件足够简单。</p><h3 id=tlb-中的内容>TLB 中的内容</h3><p>除了 VPN 和 PFN 的映射外，还有其他的一些数据，TLB 既然是转换关系的缓存，那么最好直接放页表中的 entry，即 PTE 的内容，valid bit / protection bit / dirty bit 等，另外还有 address-space identifier，下面会说到这个东西。</p><h3 id=上下文切换时-tlb-的行为>上下文切换时 TLB 的行为</h3><p>当上下文切换时，TLB 中的地址空间就换成了另一个进程的，那么一定会 cache miss，为了解决这个问题，加入了 <strong>ASID</strong> (address-space identifier)，用来标识映射关系所属的地址空间。当然 TLB 内的空间也不是无限的，就像普通的缓存一样，也要有置换策略，比如 LRU，但是 LRU 的问题在于如果进程执行的是循环读取数组之类的，那么 LRU 可能没什么意义(读取 n+1 的数据，但是 TLB 中只有 n 个映射对)，另一种策略是随机置换，实现比较简单。</p><h2 id=多级页表>多级页表</h2><p>为了解决页表占用内存过大的问题，采用多级页表的机制。</p><p>page directory 可以理解为页表的目录项，先找到虚拟地址对应的 page directory，然后从 page directory 中找到其中一部分 page table， 如果一个 page directory 中没有任何实际分配的映射关系，那么这个 page directory 就不需要分配内存，能够极大节省内存。</p><p>另外，使用 page directory 以后，页表的数据结构不再是数组了，页表直接放在内存页中。</p><p>只有一级 page directory 也有问题，以 32 位 机器，page 大小 4K 为例，地址空间为 4GB，那么进程最多有 1M 个内存页，假设 PTE 大小为 4 byte，那么一个内存页能放置 1K 个 PTE，单个进程需要 1K 个页来存放页表，而 1K 个页的上一层 page directory 需要 4KB，正好一个内存页能放下。如果是 64 位机器呢？需要好多个 page directory，那么 page directory 又存在了 page table 浪费内存的问题。所以，解决方式是再加一层 page directory 的 page directory，也就是现代操作系统使用的多级页表（和 Colossus 解决元数据的问题有些相似，直接放在 BigTable 上，而 BigTable 又构建于 Colossus 之上）。</p><p>多级页表只是解决页表内存占用过多的一种方式，还有一种方式是翻转页表，不是 VPN -> PFN 的关系，而是 PFN -> (pid, VPN) 的关系，即只记录物理页中存放的数据是哪个进程的哪个虚拟页，但是缺点是需要遍历这个翻转页表，找到目标虚拟地址。</p><h2 id=内存置换>内存置换</h2><p>解决了页表占用内存过多的问题以后，我们来解决最后一个问题，每个进程都有很大的地址空间，但是物理内存是有限的，如果给进程一个“实际”的假象，它确实能够申请到这么多内存？这就是内存置换技术。</p><p>在磁盘上有一块空间，当物理内存不够用的时候，将一些没有正在使用的内存页保存到磁盘上，当需要的时候，再从磁盘加载，虽然物理比较小，但是磁盘空间是很大的。</p><p>之前提到过 PTE （Page Table Entry) 中包含有一个 bit 叫做 present bit，现在用到了，这个位表示内存页是否存在于物理内存中，如果不存在，那么操作系统抛出一个缺页中断 (Page Fault)，此时缺页中断处理器去把 swap 空间的页取回到内存中。</p><p>为了避免访存时出现缺页执行 IO，操作系统会使用后台线程来提前置换内存页，当空闲内存页小于 low watermark 低水位时就启动线程开始将一些空闲内存页写入 swap 空间，大于 high watermark 高水位后停止。</p><h3 id=置换策略>置换策略</h3><p>物理内存页可以看做是虚拟内存页的“缓存”，而置换策略的目标是，最大化缓存命中率。</p><ul><li>最优策略：置换将来最远使用的 page，只是理想化的概念，实际上未来不可预知，这个策略只是一个对照的指标，实际的策略应当尽力接近这个指标</li><li>FIFO</li><li>Random</li><li>LRU：和调度策略一样，从历史的经验中学习到的东西才是有效的，通过一些测试，暴露出了 LRU 的致命问题：比如物理页有 49 个，虚拟页有 50 个，进程循环访问虚拟页，那么命中率一直是 0</li></ul><h4 id=approximating-lru>Approximating LRU</h4><p>近似 LRU，LRU 的问题是每次访问都需要记录访问时间，置换页的时候需要遍历所有物理页，找到最久未访问的页，操作太重量级了，那么近似 LRU 就是不置换“最久”未使用的，只是近似。每个物理页都有一个 bit 叫做 use bit 或者 reference bit，表示这个页最近访问过。</p><p>简单介绍了一种叫 clock 算法的策略，置换时从某个页面开始，如果这个页面的 use bit 是 0，那么置换它，否则置为 0 然后往下面继续找，直到找到 use bit 是 0 的页，最坏的情况就是遍历了所有的页。这只是一种通过 use bit 实现近似 LRU 的策略，其他通过定期重置 use bit 为 0 的策略也是可以实现的。即使它达不到 LRU 的命中率，也可以非常接近，并且避免了 LRU 的缺点。</p><p>另外，dirty bit 可以优化这个策略，因为 IO 是重量级操作，置换一个未被修改的页只需要内存操作就可以了，但是一个 dirty page 就需要写磁盘，因此优先置换非 dirty bit 是一个优化点。</p><h4 id=其他策略>其他策略</h4><p>其他策略包括预取，比如代码块很可能被批量执行，还有批量置换，这样写磁盘快点。</p><p>还有内存压力太大的时候该怎么办，可以直接停止一些内存密集型应用，或者干脆杀死它，但是可能会导致其他问题。</p></div><div class=post-footer></div></article></main></body></html>